# LLM Adapters
Finetuning LLMs on a corpus of legal documents can significantly improve the model's performance.

## Adapters
Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
P-Tuning: GPT Understands, Too
Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning

## Bottleneck adapters
- AdapterH: Parameter-Efficient Transfer Learning for NLP
- AdapterP: GMAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer
- Parallel: TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING

## Parallel adapters

## LORA
A common and effective technique to train adapters is Low-Rank Adaptation (LoRA).

Variations
- DORA
- AdaLoRA
- QLoRA 

### Research
- LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS  https://arxiv.org/pdf/2106.09685.pdf


# Resources
- https://medium.com/yugen-ai-technology-blog/efficient-llm-fine-tuning-lora-dora-and-apples-innovative-approach-ea1b0b31c0a7
- https://github.com/AGI-Edgerunners/LLM-Adapters
- https://docs.vllm.ai/en/latest/features/lora.html