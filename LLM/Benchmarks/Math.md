# Benchmarks

## Math

### GSM8K
GSM8K is a dataset of 8500 grade school math problems. To reach the final answer, the models must perform a sequence–between 2 and 8 steps–of elementary calculations using basic arithmetic operations like +, −, ×, and ÷. A top middle school student should be able to solve every problem. However, even the largest models often struggle to perform these multi-step mathematical tasks.  
https://arxiv.org/abs/2110.14168

### MATH
The MATH benchmark evaluates the mathematical reasoning capabilities of LLMs. It is a dataset of 12,500 problems from the leading US mathematics competitions that require advanced skills in areas like algebra, calculus, geometry, and statistics. Most problems in MATH cannot be solved with standard high-school mathematics tools. Instead, they require problem-solving techniques and heuristics.  
https://arxiv.org/abs/2103.03874

### MathEval
MathEval is intended to thoroughly evaluate the mathematical capabilities of LLMs. Its developers meant MathEval to be the standard reference for comparing the mathematical abilities of models.  
https://github.com/math-eval/MathEval