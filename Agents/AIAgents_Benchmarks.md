# Agent Benchmarking 


## Evaluation
- Exactly Match (EM)
- Multitask Accuracy
- Generalization
- Correctness
- Fluency
- Accuracy
- Coherence
- Knowledge grounding
- Contextual relevance
- Understand and execute complex instructions
- Success rate in answering questions

## Tools

### VisualAgentBench

### GAIA

### ToolBench

### HumanEval

### MultiAgentBench
MultiAgentBench is a modular and extensible framework designed to facilitate the development, testing, and evaluation of multi-agent systems leveraging Large Language Models (LLMs).
https://github.com/MultiagentBench/MARBLE


# Resources
- https://github.com/shizhl/Multi-Agent-Papers