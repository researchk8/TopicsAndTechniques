# AIInferencing
Artificial intelligence (AI) inference is the ability of trained AI models to recognize patterns and draw conclusions from information that they havenâ€™t seen before.
In the field of artificial intelligence (AI), inference is the process that a trained machine learning model* uses to draw conclusions from brand-new data. An AI model capable of making inferences can do so without examples of the desired result. In other words, inference is an AI model in action.


The difference between AI inference and machine learning. Though closely related, AI inference and machine learning (ML) are two different steps in the AI model lifecycle. 
- Machine learning is the process of using training data and algorithms, through the process of supervised learning, to enable AI to imitate the way humans learn, gradually improving its accuracy.
- AI inference is the process of applying what the AI model learned through ML to decide, predict or conclude from data.



## Platforms

### Groq


### Fireworks

### OctoAI

### Deepinfra

### together.ai
Train, fine-tune, and run inference on AI models blazing fast, at low cost, and at production scale.  
https://www.together.ai/

### deepset-ai haystack
AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data.
https://github.com/deepset-ai/haystack

# Resources
- https://www.cloudflare.com/learning/ai/inference-vs-training/
- https://www.ibm.com/think/topics/ai-inference